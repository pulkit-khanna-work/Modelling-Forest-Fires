{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2943d76b-05e6-427d-9994-d6e56cec12b4",
   "metadata": {},
   "source": [
    "# Building Pipelines (Simplifying Model Deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab58ee7-3962-435a-9f7e-f35741e601ca",
   "metadata": {},
   "source": [
    "#### Here we will demonstrate how to build pipeline for our first setup i.e. STMFWI using our best model with most optimum parameters and predicting the outcome variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec21e09-96fd-4549-923c-31919630d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Necessary Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error \n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import joblib\n",
    "\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91346d67-c3d8-4e29-850a-54320003b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load split file\n",
    "\n",
    "Xy_train = pd.read_csv('./forest_fires_deploy_train.csv', sep=',')\n",
    "Xy_test = pd.read_csv('./forest_fires_deploy_test.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe458fa-14f6-4f47-abb3-3e0d5a2145f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(Xy_train.drop(columns = 'area').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e9103c-57c9-46b2-b12a-bae872b0c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating outcome and explanatory variables into their resp. datasets\n",
    "\n",
    "X_train = pd.DataFrame(Xy_train[features])\n",
    "y_train = pd.DataFrame(Xy_train['area'])\n",
    "X_test = pd.DataFrame(Xy_test[features])\n",
    "y_test = pd.DataFrame(Xy_test['area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32346c8a-91b1-4e5d-a4ce-1fafd6151524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the outcome variable since our model algorithms expects it as 1-d array\n",
    "\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8084e472-bbd9-4bbc-871a-182ce0fb1c18",
   "metadata": {},
   "source": [
    "# Pipeline (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77d3a2-69cb-4e15-83a1-4ecc7d0c47e3",
   "metadata": {},
   "source": [
    "## Dropping Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c461d64a-9759-4d14-8128-f4e2fb20aa7c",
   "metadata": {},
   "source": [
    "#### As we will be dropping some columns in our data set, we will create a custom function transformer 'AttributeDeleter' that can be fed to our pipeline.\n",
    "\n",
    "#### Note: The same operation can be performed using FunctionTransformer as well but for demonstration purpose I have used the base class here. This method is generally useful when we have to perform both fit and transform, e.g. when performing scaling, box-cox transformations etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f9d40b5-9e0d-472f-8917-52596c8ec87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributeDeleter():\n",
    "    \"Enter list of columns you want to delete\"\n",
    "    def __init__(self,columns):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "        \n",
    "    def transform(self, X, y = None):\n",
    "        return X.drop(self.columns, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175f3bc-b83c-4dd2-8069-12db1bd025da",
   "metadata": {},
   "source": [
    "#### We check from our transformation section for the test dataset (Analysis First Part/File) that we are removing 'rain' column from our dataset.\n",
    "\n",
    "#### Note: We can check and confirm for any discrepancy(which columns were removed) by comparing the columns of the initial file and the final wrangled file. \n",
    "#### We don't remove 'area' column using the transformer as here we are assuming that for any new data we won't be having 'area' variable column, and thus for our train and test datasets its better to remove manually before fitting them in our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68fffc-6d78-4c3e-a41a-19c3232bed10",
   "metadata": {},
   "source": [
    "## Log (1+x) Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd56c0a3-6f48-4d14-8d57-cda26934ffb8",
   "metadata": {},
   "source": [
    "#### Next we perform log (1+x) transformation which is a stateless transformation. Also, we had created a custom function to transform positively and negatively skewed columns differently. To be able to fit it to our pipeline, we will make use of 'FunctionTransformer' function which will take our custom 'log1p_tranform' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d81ccb8-b04a-4b2d-a4b7-00fd9ac5a18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['RH'], dtype='object')\n",
      "Index(['DC', 'FFMC'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Calculating skewness of dataset\n",
    "\n",
    "skew_limit = 0.75\n",
    "skew_vals = X_train.select_dtypes(exclude = 'object').skew().drop(['X','Y','rain'])\n",
    "skew_cols = skew_vals[abs(skew_vals) > skew_limit].sort_values(ascending = False).index.values\n",
    "\n",
    "pos_skew = X_train[skew_cols].agg(['skew']).transpose().query('skew > 0').index\n",
    "neg_skew = X_train[skew_cols].agg(['skew']).transpose().query('skew < 0').index\n",
    "\n",
    "print(pos_skew)\n",
    "print(neg_skew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2946570-3943-430b-85a3-34d808d335d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log1p_transform(X):\n",
    "    \"\"\" Log(1+x) transformation for positive and negatively skewed data\n",
    "    with skew limit set to 0.75\n",
    "    \"\"\"    \n",
    "    pos_skew = ['RH']\n",
    "    neg_skew = ['DC', 'FFMC']\n",
    "    \n",
    "    for cols in pos_skew:\n",
    "        X[cols] = np.log1p(X[cols])\n",
    "\n",
    "    # Reversing distribution before applying log transform\n",
    "    for cols in neg_skew:\n",
    "        X[cols] = np.log1p(max(X[cols] + 1) - X[cols])\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d6eecd-c830-4ee7-91bc-4bc6ec54284d",
   "metadata": {},
   "source": [
    "##  Cyclical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd8aab-23e9-42f1-b87c-76944771b5c9",
   "metadata": {},
   "source": [
    "#### We then had performed cyclical encoding for our 'day' and 'month' variables. Again we will make use of 'FunctionTransformer' function which will take our custom 'cyclical_encoding' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dd10b60-09dc-4a74-8893-9a4111629dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_encoding(X):    \n",
    "    cleanup_nums = { \"day\": {'fri': 5, 'tue': 2, 'sat': 6, 'sun': 7, 'mon': 1, 'wed': 3, 'thu': 2},\n",
    "                \"month\": {'mar': 3, 'oct': 10, 'aug': 8, 'sep': 9, 'apr': 4, 'jun': 5, 'jul': 6,\n",
    "                          'feb': 2, 'jan': 1, 'dec': 12, 'may': 5, 'nov': 11}\n",
    "               }\n",
    "    X = X.replace(cleanup_nums)\n",
    "    \n",
    "    X['day_sin'] = np.sin((X.day) * (2 * np.pi/7))\n",
    "    X['day_cos'] = np.cos((X.day) * (2 * np.pi/7))\n",
    "    X['month_sin'] = np.sin((X.month) * (2 * np.pi/12))\n",
    "    X['month_cos'] = np.cos((X.month) * (2 * np.pi/12))\n",
    "    X = X.drop(columns = ['day' ,'month'])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c8ae20-578b-4d48-9f78-50140343a697",
   "metadata": {},
   "source": [
    "#### We can build our first pipeline for the first part now, keeping in mind the order where relevant/important of different operations performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93ba2b27-f137-4c34-a880-1134a1d2bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_pipeline = Pipeline([('del_attributes', AttributeDeleter(['rain'])), \n",
    "                              ('log1p', FunctionTransformer(log1p_transform)),\n",
    "                              ('cyc_enc', FunctionTransformer(cyclical_encoding))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c64f2-2daf-432e-b69a-f6f7a4234aed",
   "metadata": {},
   "source": [
    "#### As we are removing our outcome variable 'area' manually here before fitting the data in pipeline, we shouldn't forget about any transformation/operation performed on our test variables earlier which we will have to now separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f47ee0fb-4647-46fc-8ed2-cd86c262d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log1p(y_train)\n",
    "y_test = np.log1p(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6452c2-861e-4168-b8ab-545a84a5b9a3",
   "metadata": {},
   "source": [
    "# Pipeline (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ef793-1533-4861-8efb-967817103411",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7665bad-8435-4899-b8fa-620a619c998b",
   "metadata": {},
   "source": [
    "#### In the second part, we then performed Robust Scaling to normalize our data features. We can select the specific columns we want to scale with the help of ColumnTransformer and then add it to our pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb79d1a4-3dfa-4739-b823-f7167bc70ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_cols = ['FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind']\n",
    "\n",
    "preprocess = ColumnTransformer(remainder = 'passthrough', #passthough features not listed\n",
    "                               transformers = [('rb_scaler', RobustScaler(), scale_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59ed79-5921-4b3b-a38e-bfdbc788df94",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40651a-7aa0-4ba7-a450-d61bb727ac86",
   "metadata": {},
   "source": [
    "#### Now our best model was SVR(Gaussian-RBF kernel). We can load our saved model, extract the best parameters and fit SVR model to the pipeline with best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1ee1021-bbca-4586-80a6-83a4f90aa0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVR = joblib.load('best_model_SVR_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f47fa83-b646-4f9f-aefa-1a74f6bae138",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_pipeline = Pipeline([('rb_scaler', preprocess), \n",
    "                              ('SVR_model', SVR(**model_SVR.best_params_))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd75fb7e-89d8-4d43-be9a-a9b727b362d3",
   "metadata": {},
   "source": [
    "# Full Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d4b02-4412-44ca-b8d3-f524409b5f4a",
   "metadata": {},
   "source": [
    "#### Our full pipeline can be made by combining the two other pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cda4fecc-2e02-457b-8832-4abdd97088ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = Pipeline([('wrangled', wrangled_pipeline), \n",
    "                          ('modelling', modelling_pipeline)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18de507e-5c6d-4402-af7f-8b1ae374134d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85afc2-5aec-448e-84b7-2f60a8580d03",
   "metadata": {},
   "source": [
    "#### After our pipeline is  set, we can finally train our model by using fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bc3d6c3-b4d7-4211-ae8a-89fe8abfafeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = full_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb5693e4-42bd-40ac-8946-00ba431cdcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('wrangled',\n",
       "                 Pipeline(steps=[('del_attributes',\n",
       "                                  <__main__.AttributeDeleter object at 0x7f802f41a8b0>),\n",
       "                                 ('log1p',\n",
       "                                  FunctionTransformer(func=<function log1p_transform at 0x7f802f4248b0>)),\n",
       "                                 ('cyc_enc',\n",
       "                                  FunctionTransformer(func=<function cyclical_encoding at 0x7f802f424ca0>))])),\n",
       "                ('modelling',\n",
       "                 Pipeline(steps=[('rb_scaler',\n",
       "                                  ColumnTransformer(remainder='passthrough',\n",
       "                                                    transformers=[('rb_scaler',\n",
       "                                                                   RobustScaler(),\n",
       "                                                                   ['FFMC',\n",
       "                                                                    'DMC', 'DC',\n",
       "                                                                    'ISI',\n",
       "                                                                    'temp',\n",
       "                                                                    'RH',\n",
       "                                                                    'wind'])])),\n",
       "                                 ('SVR_model', SVR(C=3, gamma=0.03))]))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f901db6d-40c3-4f36-8041-b3932681d525",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed28df7-9c99-450f-850f-02a1041de41f",
   "metadata": {},
   "source": [
    "#### Now our model is ready and we can finally start predicting test observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7e049db-e23a-410d-a8c5-7cd0378b0bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = full_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bea46190-6a0b-48fb-a27b-9f8af19b6bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.64368803,  0.41610555,  0.80751326,  0.7841918 ,  0.36785279,\n",
       "        0.53324918,  1.51292203, -0.13628372,  0.43089379,  1.19911324])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aeb6c1-41b5-4e03-b3a3-8c02d3ad0892",
   "metadata": {},
   "source": [
    "#### We can make a function to calcualte 'MAE' and 'RMSE' after predicting values using pipeline which can then be sent back to the user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f65eb5e7-2f8d-477e-810c-7cd1b2798e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>1.031718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE</th>\n",
       "      <td>1.413421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           SVR\n",
       "MAE   1.031718\n",
       "RMSE  1.413421"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = dict()\n",
    "pred_metrics = dict()\n",
    "\n",
    "pred['MAE'] = mean_absolute_error(y_test, test)\n",
    "pred['RMSE'] = mean_squared_error(y_test, test, squared = False)\n",
    "                \n",
    "pred_metrics['SVR'] = pd.Series(pred)\n",
    "pd.DataFrame(pred_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b448fb7-58d2-4b62-af30-2baebd9ba4e3",
   "metadata": {},
   "source": [
    "# Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f7f654-1020-4b06-8fb0-47feba52af9d",
   "metadata": {},
   "source": [
    "#### After the pipeline is set we can serve our model into production. There are different approaches each having its own benefits and tradeoffs -> Batch, Realtime (Database Trigger, Pub/Sub, Web-service, inApp).\n",
    "#### Here we are using sklearn pipeline, so we will deploy a flask, django or fastAPI application through a docker container."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
